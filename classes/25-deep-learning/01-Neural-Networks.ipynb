{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks \n",
    "- This was adopted from the PyTorch Tutorials. \n",
    "- http://pytorch.org/tutorials/beginner/pytorch_with_examples.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks \n",
    "- Neural networks are the foundation of deep learning, which has revolutionized the \n",
    "\n",
    "```In the mathematical theory of artificial neural networks, the universal approximation theorem states[1] that a feed-forward network with a single hidden layer containing a finite number of neurons (i.e., a multilayer perceptron), can approximate continuous functions on compact subsets of Rn, under mild assumptions on the activation function.```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Fake Data\n",
    "- `D_in` is the number of dimensions of an input varaible.\n",
    "- `D_out` is the number of dimentions of an output variable.\n",
    "- Here we are learning some special \"fake\" data that represents the xor problem. \n",
    "- Here, the dv is 1 if either the first or second variable is \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      " [[0 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]] \n",
      " Output data:\n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "#This is our independent and dependent variables. \n",
    "x = np.array([ [0,0,0],[1,0,0],[0,1,0],[0,0,0] ])\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "print(\"Input data:\\n\",x,\"\\n Output data:\\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Neural Network \n",
    "- Here we are going to build a neural network with 2 hidden layers. \n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=83832)\n",
    "#D_in is the number of input variables. \n",
    "#H is the hidden dimension.\n",
    "#D_out is the number of dimensions for the output. \n",
    "D_in, H, D_out = 3, 3, 1\n",
    "\n",
    "# Randomly initialize weights og out 2 hidden layer network.\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "bias = np.random.randn(H, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn the Appropriate Weights via Backpropogation\n",
    "- Learning rate adjust how quickly the model will adjust parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.160537076323\n",
      "1 0.153595453821\n",
      "2 0.146975821295\n",
      "3 0.140662873332\n",
      "4 0.134642046866\n",
      "5 0.128899484146\n",
      "6 0.123421997627\n",
      "7 0.118197036687\n",
      "8 0.11321265606\n",
      "9 0.108457485896\n",
      "10 0.103920703363\n",
      "11 0.0995920057013\n",
      "12 0.0954615846471\n",
      "13 0.0915201021691\n",
      "14 0.0877586674284\n",
      "15 0.0841688149077\n",
      "16 0.0807424836442\n",
      "17 0.0774719975089\n",
      "18 0.074350046475\n",
      "19 0.0713696688267\n",
      "20 0.0685242342555\n",
      "21 0.0658074278016\n",
      "22 0.0632132345924\n",
      "23 0.0607359253406\n",
      "24 0.0583700425598\n",
      "25 0.0561103874624\n",
      "26 0.0539520075041\n",
      "27 0.0518901845415\n",
      "28 0.0499204235731\n",
      "29 0.0480384420318\n",
      "30 0.0462401596028\n",
      "31 0.0445216885394\n",
      "32 0.042879324452\n",
      "33 0.0413095375454\n",
      "34 0.0398089642841\n",
      "35 0.0383743994617\n",
      "36 0.0370027886556\n",
      "37 0.0356912210476\n",
      "38 0.034436922592\n",
      "39 0.0332372495143\n",
      "40 0.032089682123\n",
      "41 0.030991818921\n",
      "42 0.0299413709995\n",
      "43 0.0289361567028\n",
      "44 0.0279740965484\n",
      "45 0.0270532083915\n",
      "46 0.0261716028219\n",
      "47 0.0253274787804\n",
      "48 0.0245191193855\n",
      "49 0.0237448879602\n",
      "50 0.0230032242474\n",
      "51 0.022292640807\n",
      "52 0.021611719584\n",
      "53 0.0209591086412\n",
      "54 0.0203335190457\n",
      "55 0.0197337219056\n",
      "56 0.0191585455455\n",
      "57 0.018606872818\n",
      "58 0.0180776385411\n",
      "59 0.0175698270584\n",
      "60 0.0170824699145\n",
      "61 0.01661464364\n",
      "62 0.0161654676424\n",
      "63 0.0157341021955\n",
      "64 0.0153197465251\n",
      "65 0.014921636984\n",
      "66 0.0145390453138\n",
      "67 0.014171276989\n",
      "68 0.0138176696382\n",
      "69 0.0134775915407\n",
      "70 0.0131504401935\n",
      "71 0.0128356409457\n",
      "72 0.0125326456973\n",
      "73 0.0122409316596\n",
      "74 0.0119600001727\n",
      "75 0.0116893755804\n",
      "76 0.0114286041559\n",
      "77 0.01117725308\n",
      "78 0.0109349094653\n",
      "79 0.010701179428\n",
      "80 0.0104756872016\n",
      "81 0.0102580742937\n",
      "82 0.010047998681\n",
      "83 0.00984513404276\n",
      "84 0.00964916903003\n",
      "85 0.00945980656902\n",
      "86 0.00927676319706\n",
      "87 0.00909976842975\n",
      "88 0.00892856415757\n",
      "89 0.00876290407083\n",
      "90 0.0086025531114\n",
      "91 0.00844728695017\n",
      "92 0.0082968914888\n",
      "93 0.00815116238488\n",
      "94 0.00800990459915\n",
      "95 0.00787293196389\n",
      "96 0.00774006677152\n",
      "97 0.00761113938229\n",
      "98 0.00748598785037\n",
      "99 0.00736445756728\n",
      "100 0.00724640092204\n",
      "101 0.00713167697713\n",
      "102 0.0070201511595\n",
      "103 0.00691169496606\n",
      "104 0.00680618568284\n",
      "105 0.00670350611725\n",
      "106 0.00660354434279\n",
      "107 0.00650619345568\n",
      "108 0.00641135134276\n",
      "109 0.00631892046026\n",
      "110 0.00622880762281\n",
      "111 0.00614092380232\n",
      "112 0.00605518393613\n",
      "113 0.00597150674419\n",
      "114 0.00588981455463\n",
      "115 0.00581003313754\n",
      "116 0.00573209154639\n",
      "117 0.00565592196689\n",
      "118 0.00558145957281\n",
      "119 0.00550864238855\n",
      "120 0.00543741115806\n",
      "121 0.00536770921985\n",
      "122 0.00529948238784\n",
      "123 0.00523267883766\n",
      "124 0.00516724899834\n",
      "125 0.00510314544891\n",
      "126 0.00504032281989\n",
      "127 0.00497873769929\n",
      "128 0.004918348543\n",
      "129 0.00485911558932\n",
      "130 0.00480100077749\n",
      "131 0.00474396766991\n",
      "132 0.00468798137806\n",
      "133 0.00463300849183\n",
      "134 0.00457901701204\n",
      "135 0.00452597628624\n",
      "136 0.00447385694733\n",
      "137 0.00442263085511\n",
      "138 0.00437227104053\n",
      "139 0.00432275165248\n",
      "140 0.00427404790707\n",
      "141 0.00422613603928\n",
      "142 0.00417899325678\n",
      "143 0.00413259769598\n",
      "144 0.00408692837997\n",
      "145 0.0040419651786\n",
      "146 0.00399768877019\n",
      "147 0.00395408060521\n",
      "148 0.00391112287146\n",
      "149 0.003868798461\n",
      "150 0.00382709093853\n",
      "151 0.00378598451122\n",
      "152 0.00374546399999\n",
      "153 0.00370551481208\n",
      "154 0.00366612291487\n",
      "155 0.00362727481093\n",
      "156 0.00358895751425\n",
      "157 0.00355115852746\n",
      "158 0.00351386582023\n",
      "159 0.00347706780857\n",
      "160 0.00344075333512\n",
      "161 0.00340491165037\n",
      "162 0.00336953239467\n",
      "163 0.00333460558114\n",
      "164 0.00330012157933\n",
      "165 0.00326607109964\n",
      "166 0.00323244517845\n",
      "167 0.00319923516395\n",
      "168 0.00316643270257\n",
      "169 0.00313402972611\n",
      "170 0.00310201843939\n",
      "171 0.00307039130848\n",
      "172 0.00303914104953\n",
      "173 0.00300826061799\n",
      "174 0.00297774319848\n",
      "175 0.00294758219495\n",
      "176 0.00291777122145\n",
      "177 0.00288830409318\n",
      "178 0.00285917481807\n",
      "179 0.00283037758866\n",
      "180 0.0028019067744\n",
      "181 0.00277375691425\n",
      "182 0.00274592270965\n",
      "183 0.00271839901782\n",
      "184 0.00269118084536\n",
      "185 0.00266426334208\n",
      "186 0.00263764179519\n",
      "187 0.00261131162373\n",
      "188 0.00258526837321\n",
      "189 0.00255950771055\n",
      "190 0.0025340254192\n",
      "191 0.00250881739454\n",
      "192 0.00248387963938\n",
      "193 0.00245920825982\n",
      "194 0.00243479946113\n",
      "195 0.00241064954392\n",
      "196 0.00238675490047\n",
      "197 0.00236311201117\n",
      "198 0.00233971744118\n",
      "199 0.00231656783723\n",
      "200 0.00229365992449\n",
      "201 0.00227099050371\n",
      "202 0.00224855644832\n",
      "203 0.00222635470186\n",
      "204 0.00220438227533\n",
      "205 0.00218263624475\n",
      "206 0.00216111374888\n",
      "207 0.00213981198689\n",
      "208 0.0021187282163\n",
      "209 0.00209785975087\n",
      "210 0.00207720395869\n",
      "211 0.00205675826026\n",
      "212 0.00203652012674\n",
      "213 0.0020164870782\n",
      "214 0.00199665668201\n",
      "215 0.00197702655126\n",
      "216 0.00195759434323\n",
      "217 0.00193835775799\n",
      "218 0.00191931453701\n",
      "219 0.00190046246184\n",
      "220 0.00188179935284\n",
      "221 0.00186332306798\n",
      "222 0.00184503150168\n",
      "223 0.0018269225837\n",
      "224 0.00180899427808\n",
      "225 0.00179124458211\n",
      "226 0.00177367152537\n",
      "227 0.00175627316877\n",
      "228 0.00173904760369\n",
      "229 0.00172199295106\n",
      "230 0.00170510736061\n",
      "231 0.00168838900999\n",
      "232 0.00167183610411\n",
      "233 0.0016554468743\n",
      "234 0.00163921957769\n",
      "235 0.00162315249653\n",
      "236 0.00160724393748\n",
      "237 0.00159149223108\n",
      "238 0.00157589573107\n",
      "239 0.00156045281388\n",
      "240 0.00154516187802\n",
      "241 0.00153002134362\n",
      "242 0.00151502965184\n",
      "243 0.00150018526444\n",
      "244 0.0014854866633\n",
      "245 0.00147093234992\n",
      "246 0.00145652084504\n",
      "247 0.00144225068817\n",
      "248 0.00142812043724\n",
      "249 0.00141412866812\n",
      "250 0.00140027397434\n",
      "251 0.00138655496667\n",
      "252 0.00137297027276\n",
      "253 0.00135951853685\n",
      "254 0.00134619841938\n",
      "255 0.00133300859672\n",
      "256 0.00131994776087\n",
      "257 0.0013070146191\n",
      "258 0.00129420789376\n",
      "259 0.0012815263219\n",
      "260 0.00126896865511\n",
      "261 0.00125653365916\n",
      "262 0.00124422011383\n",
      "263 0.00123202681262\n",
      "264 0.00121995256253\n",
      "265 0.00120799618384\n",
      "266 0.00119615650987\n",
      "267 0.00118443238679\n",
      "268 0.0011728226734\n",
      "269 0.00116132624092\n",
      "270 0.0011499419728\n",
      "271 0.00113866876456\n",
      "272 0.00112750552355\n",
      "273 0.00111645116882\n",
      "274 0.0011055046309\n",
      "275 0.00109466485169\n",
      "276 0.00108393078422\n",
      "277 0.00107330139257\n",
      "278 0.00106277565164\n",
      "279 0.00105235254706\n",
      "280 0.00104203107497\n",
      "281 0.00103181024196\n",
      "282 0.00102168906487\n",
      "283 0.00101166657065\n",
      "284 0.00100174179626\n",
      "285 0.000991913788526\n",
      "286 0.000982181603989\n",
      "287 0.000972544308809\n",
      "288 0.000963000978627\n",
      "289 0.000953550698449\n",
      "290 0.00094419256253\n",
      "291 0.000934925674254\n",
      "292 0.000925749146028\n",
      "293 0.000916662099169\n",
      "294 0.000907663663795\n",
      "295 0.000898752978721\n",
      "296 0.000889929191359\n",
      "297 0.000881191457608\n",
      "298 0.000872538941763\n",
      "299 0.000863970816414\n",
      "300 0.00085548626235\n",
      "301 0.000847084468466\n",
      "302 0.00083876463167\n",
      "303 0.000830525956792\n",
      "304 0.000822367656498\n",
      "305 0.0008142889512\n",
      "306 0.000806289068969\n",
      "307 0.000798367245454\n",
      "308 0.000790522723797\n",
      "309 0.000782754754551\n",
      "310 0.000775062595601\n",
      "311 0.000767445512082\n",
      "312 0.000759902776306\n",
      "313 0.000752433667681\n",
      "314 0.00074503747264\n",
      "315 0.00073771348456\n",
      "316 0.000730461003696\n",
      "317 0.000723279337105\n",
      "318 0.000716167798576\n",
      "319 0.000709125708559\n",
      "320 0.000702152394099\n",
      "321 0.000695247188762\n",
      "322 0.000688409432576\n",
      "323 0.000681638471957\n",
      "324 0.000674933659649\n",
      "325 0.000668294354658\n",
      "326 0.000661719922186\n",
      "327 0.000655209733574\n",
      "328 0.000648763166234\n",
      "329 0.000642379603591\n",
      "330 0.000636058435023\n",
      "331 0.000629799055799\n",
      "332 0.000623600867024\n",
      "333 0.000617463275576\n",
      "334 0.000611385694054\n",
      "335 0.000605367540715\n",
      "336 0.000599408239425\n",
      "337 0.000593507219599\n",
      "338 0.000587663916146\n",
      "339 0.000581877769417\n",
      "340 0.000576148225152\n",
      "341 0.000570474734424\n",
      "342 0.000564856753592\n",
      "343 0.000559293744243\n",
      "344 0.000553785173145\n",
      "345 0.000548330512196\n",
      "346 0.000542929238374\n",
      "347 0.000537580833688\n",
      "348 0.000532284785126\n",
      "349 0.000527040584611\n",
      "350 0.000521847728952\n",
      "351 0.000516705719795\n",
      "352 0.000511614063578\n",
      "353 0.000506572271483\n",
      "354 0.000501579859393\n",
      "355 0.000496636347844\n",
      "356 0.000491741261982\n",
      "357 0.000486894131518\n",
      "358 0.000482094490684\n",
      "359 0.000477341878188\n",
      "360 0.000472635837175\n",
      "361 0.000467975915181\n",
      "362 0.000463361664092\n",
      "363 0.000458792640102\n",
      "364 0.000454268403673\n",
      "365 0.000449788519492\n",
      "366 0.000445352556433\n",
      "367 0.000440960087514\n",
      "368 0.000436610689862\n",
      "369 0.000432303944669\n",
      "370 0.000428039437154\n",
      "371 0.000423816756527\n",
      "372 0.000419635495951\n",
      "373 0.000415495252499\n",
      "374 0.000411395627125\n",
      "375 0.000407336224618\n",
      "376 0.000403316653574\n",
      "377 0.000399336526352\n",
      "378 0.000395395459046\n",
      "379 0.000391493071441\n",
      "380 0.000387628986984\n",
      "381 0.000383802832749\n",
      "382 0.000380014239397\n",
      "383 0.000376262841148\n",
      "384 0.000372548275743\n",
      "385 0.000368870184415\n",
      "386 0.00036522821185\n",
      "387 0.000361622006158\n",
      "388 0.00035805121884\n",
      "389 0.000354515504753\n",
      "390 0.000351014522083\n",
      "391 0.00034754793231\n",
      "392 0.000344115400176\n",
      "393 0.000340716593655\n",
      "394 0.000337351183924\n",
      "395 0.00033401884533\n",
      "396 0.000330719255361\n",
      "397 0.000327452094616\n",
      "398 0.000324217046775\n",
      "399 0.00032101379857\n",
      "400 0.000317842039756\n",
      "401 0.000314701463082\n",
      "402 0.000311591764263\n",
      "403 0.000308512641951\n",
      "404 0.000305463797708\n",
      "405 0.000302444935977\n",
      "406 0.000299455764057\n",
      "407 0.000296495992072\n",
      "408 0.000293565332947\n",
      "409 0.000290663502382\n",
      "410 0.000287790218821\n",
      "411 0.000284945203432\n",
      "412 0.000282128180077\n",
      "413 0.000279338875288\n",
      "414 0.000276577018238\n",
      "415 0.000273842340723\n",
      "416 0.000271134577129\n",
      "417 0.000268453464415\n",
      "418 0.000265798742081\n",
      "419 0.000263170152148\n",
      "420 0.000260567439135\n",
      "421 0.000257990350032\n",
      "422 0.000255438634279\n",
      "423 0.000252912043739\n",
      "424 0.00025041033268\n",
      "425 0.000247933257749\n",
      "426 0.000245480577948\n",
      "427 0.000243052054615\n",
      "428 0.000240647451399\n",
      "429 0.00023826653424\n",
      "430 0.000235909071344\n",
      "431 0.000233574833166\n",
      "432 0.000231263592382\n",
      "433 0.000228975123875\n",
      "434 0.000226709204709\n",
      "435 0.000224465614111\n",
      "436 0.000222244133446\n",
      "437 0.000220044546203\n",
      "438 0.00021786663797\n",
      "439 0.000215710196413\n",
      "440 0.000213575011261\n",
      "441 0.000211460874283\n",
      "442 0.000209367579267\n",
      "443 0.000207294922006\n",
      "444 0.000205242700272\n",
      "445 0.000203210713804\n",
      "446 0.000201198764281\n",
      "447 0.000199206655313\n",
      "448 0.000197234192415\n",
      "449 0.000195281182991\n",
      "450 0.000193347436317\n",
      "451 0.000191432763522\n",
      "452 0.000189536977572\n",
      "453 0.000187659893248\n",
      "454 0.000185801327133\n",
      "455 0.000183961097594\n",
      "456 0.000182139024762\n",
      "457 0.00018033493052\n",
      "458 0.00017854863848\n",
      "459 0.000176779973972\n",
      "460 0.000175028764024\n",
      "461 0.000173294837349\n",
      "462 0.000171578024323\n",
      "463 0.000169878156977\n",
      "464 0.000168195068973\n",
      "465 0.000166528595595\n",
      "466 0.000164878573729\n",
      "467 0.000163244841849\n",
      "468 0.000161627240004\n",
      "469 0.000160025609798\n",
      "470 0.00015843979438\n",
      "471 0.000156869638425\n",
      "472 0.000155314988123\n",
      "473 0.000153775691162\n",
      "474 0.000152251596713\n",
      "475 0.00015074255542\n",
      "476 0.000149248419379\n",
      "477 0.000147769042131\n",
      "478 0.000146304278643\n",
      "479 0.000144853985296\n",
      "480 0.000143418019873\n",
      "481 0.000141996241544\n",
      "482 0.00014058851085\n",
      "483 0.000139194689696\n",
      "484 0.000137814641331\n",
      "485 0.000136448230341\n",
      "486 0.000135095322631\n",
      "487 0.000133755785417\n",
      "488 0.000132429487209\n",
      "489 0.000131116297803\n",
      "490 0.000129816088262\n",
      "491 0.000128528730913\n",
      "492 0.000127254099325\n",
      "493 0.000125992068305\n",
      "494 0.000124742513882\n",
      "495 0.000123505313294\n",
      "496 0.000122280344982\n",
      "497 0.00012106748857\n",
      "498 0.000119866624862\n",
      "499 0.000118677635825\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "learning_rate = 1e-3\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y\n",
    "    h = x.dot(w1)\n",
    "\n",
    "    #A relu is just the activation.\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # Update weights\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CFully connected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        ]\n",
      " [ 0.98919177]\n",
      " [ 1.00082623]\n",
      " [ 0.        ]] \n",
      " [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred = np.maximum(x.dot(w1),0).dot(w2)\n",
    "\n",
    "print (pred, \"\\n\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Layers are Often Viewed as Unknown\n",
    "- Just a weighting matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74015787, -0.16474176, -0.48135933],\n",
       "       [ 1.23746379, -0.00148781,  0.31587215],\n",
       "       [-0.49991311, -0.18841141, -0.16494133]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#However\n",
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.16346749],\n",
       "       [-2.15990232],\n",
       "       [-1.30731163]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.72108356,  0.        ,  0.        ],\n",
       "       [ 0.72753913,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Relu just removes the negative numbers.  \n",
    "h_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
